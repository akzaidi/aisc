# 2022-01-22

- Kyle and Laria -> focus on prosaic alignment, have been doing LOTS of experiments with GPT3
- Have developed a tool that makes it very easy to generate high-quality completions with a human-in-the-loop
  - have generated over 10K pages
  - doesn't yet act like a rational inference agent, e.g., if the prompt is underspecified, it will still act like a faithful representation of the training data and add deterministic entities for what is missing and act like they were present from the context
  - the probability distribution spread are high for the generated text
- Question: is there anything that you can tune using temperature to prevent that behavior
- Showing current capabilities as well as failures is a useful endeavor
  - i.e., showcasing it's inability to handle uncertainty well
- Another trend: amplification through RL
  - grounded behavior vs self-supervised behavior, does this lead to different amplification errors
- example alignment failures from _summarization from human feedback_ paper
  - if you train too long on the reward model -> more short summaries + incoherence
  - reward model is not robust OOD
  - using KL divergence as an additional penalty seemed to mitigate this
  - is this just a bandaid üè•
- Laria has prototyped some algorithms for measuring the amount of curation needed for good quality outputs, but rigorifying this could be fun
- Deepmind: Shaking the foundations
- TruthfulQA: almost adversarial to GPT-3, showcases why GPT-3 fails (almost expected), but not yet understood as to the why